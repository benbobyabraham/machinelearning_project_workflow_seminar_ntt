{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project Pipeline\n",
    "\n",
    "## Problem statement\n",
    "## Solution\n",
    "\n",
    "To build a predictive model for churn prediction, you would need to perform the following steps:\n",
    "\n",
    "- Data Preprocessing: Clean and preprocess the data, including handling missing values, feature engineering, and data transformation.\n",
    "\n",
    "- Exploratory Data Analysis: Perform an exploratory data analysis to identify relationships between variables and identify patterns in the data.\n",
    "\n",
    "- Feature Selection: Identify the most important features that contribute to churn prediction.\n",
    "\n",
    "- Model Building: Build a model to predict churn using machine learning algorithms such as Logistic Regression, Random Forest, XGBoost, or Neural Networks.\n",
    "\n",
    "- Model Evaluation: Evaluate the performance of the model using appropriate evaluation metrics such as accuracy, recall, precision, F1 score, and ROC-AUC score.\n",
    "\n",
    "- Hyperparameter Tuning: Optimize the hyperparameters of the model using techniques such as grid search or randomized search.\n",
    "\n",
    "- Deployment: Deploy the model in a production environment, where it can be used to predict churn in real-time.\n",
    "\n",
    "I recommend using a programming language like Python and its popular data science libraries such as Pandas, Numpy, Matplotlib, Seaborn, Scikit-Learn, and Keras/TensorFlow to perform the above steps.\n",
    "\n",
    "You can refer to online tutorials and resources to learn more about these libraries and techniques. Additionally, you can refer to Kaggle or similar data science websites to access similar project codes and approaches to churn prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df = df.fillna(0) # Replace missing values with 0 or any other appropriate value\n",
    "df = df.dropna() # Drop rows with missing values\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Remove unwanted columns\n",
    "df = df.drop(['id', 'name'], axis=1)\n",
    "\n",
    "# Convert categorical data to numerical data\n",
    "df['gender'] = df['gender'].replace(['male', 'female'], [0, 1]) # Example for binary categorical data\n",
    "df = pd.get_dummies(df, columns=['city']) # Example for non-binary categorical data\n",
    "\n",
    "# Feature scaling\n",
    "df['age'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())\n",
    "\n",
    "# Feature selection\n",
    "X = df.drop(['target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data into a Pandas dataframe\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Preview the data to get a better understanding of its structure\n",
    "print(df.head())\n",
    "\n",
    "# Generate descriptive statistics of the data\n",
    "print(df.describe())\n",
    "\n",
    "# Plot a histogram of the numerical variables\n",
    "df.hist(bins=10, figsize=(20,15))\n",
    "plt.show()\n",
    "\n",
    "# Create a correlation matrix to visualize the relationships between variables\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# Plot boxplots of numerical variables to detect outliers and distribution\n",
    "sns.boxplot(data=df.select_dtypes(include='float'))\n",
    "plt.show()\n",
    "\n",
    "# Generate pairplots to see the pairwise relationships between variables\n",
    "sns.pairplot(df, hue='target_variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "This code performs the following EDA tasks:\n",
    "\n",
    "- Load the data into a Pandas dataframe\n",
    "- Preview the data to get a better understanding of its structure\n",
    "- Generate descriptive statistics of the data\n",
    "- Plot a histogram of the numerical variables\n",
    "- Create a correlation matrix to visualize the relationships between variables\n",
    "- Plot boxplots of numerical variables to detect outliers and distribution\n",
    "- Generate pairplots to see the pairwise relationships between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Selection\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Separate the target variable\n",
    "X = data.iloc[:, :-1] # Features\n",
    "y = data.iloc[:, -1]  # Target variable\n",
    "\n",
    "# Feature selection using SelectKBest\n",
    "selector = SelectKBest(score_func=chi2, k=10) # Here, we choose chi-squared test as the score function and select 10 best features\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the selected features' indices and names\n",
    "selected_features = selector.get_support(indices=True)\n",
    "feature_names = X.columns[selected_features]\n",
    "\n",
    "# Print the selected features' names\n",
    "print(feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "In this code, we first load the dataset and separate the target variable. Then, we apply the SelectKBest feature selection method using the chi2 (chi-squared) test as the score function and select the top 10 best features. After fitting the selector on the features and the target variable, we get the indices of the selected features using get_support method and use them to extract the corresponding feature names from the original feature set. Finally, we print the selected feature names. You can adjust the value of k to select more or fewer features based on your requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model Building\n",
    "**Logistic Regression Model**\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model on training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Decision Tree Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate the model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model on training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Model**\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creating the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluating the performance of the model\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Random Forest Classifier Accuracy:\", rf_accuracy)\n",
    "```\n",
    "Using the  Random Forest Classifier algorithm and training it with 100 decision trees. We then evaluate the accuracy of the model using the accuracy_score function from the sklearn.metrics library. You can also tune the hyperparameters of the model to get better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# define hyperparameters to tune\n",
    "params = {'penalty': ['l1', 'l2'],\n",
    "          'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "          'solver': ['liblinear', 'saga']}\n",
    "\n",
    "# perform grid search cross-validation\n",
    "grid_search = GridSearchCV(lr, params, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print best hyperparameters and corresponding accuracy score\n",
    "print('Best hyperparameters:', grid_search.best_params_)\n",
    "print('Accuracy score:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "We use GridSearchCV to perform a grid search cross-validation over a range of hyperparameters for our logistic regression model. The hyperparameters we tune are penalty, C, and solver. The best hyperparameters and corresponding accuracy score are printed at the end.\n",
    "\n",
    "As for suggestions to take into account, it's important to keep in mind that hyperparameter tuning can be a time-consuming process, especially with larger datasets or more complex models. It's also important to strike a balance between underfitting (where the model is too simple and doesn't capture the complexity of the data) and overfitting (where the model is too complex and fits the noise in the data rather than the underlying patterns). It's a good idea to start with a simple model and gradually increase its complexity while monitoring its performance on a validation set. Additionally, consider using techniques like regularization (e.g., L1 or L2 regularization) to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Deployment\n",
    "\n",
    "The deployment of a machine learning model depends on the context of the project and the requirements of the application. However, I can provide a general guide on how to deploy a machine learning model using Flask as a web framework.\n",
    "\n",
    "Step 1: Export the Trained Model\n",
    "The first step is to export the trained model in a format that can be used in the web application. Typically, the model is saved as a file in a format such as .pkl or .h5. In this example, we will save the model as a pickle file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Export the logistic regression model\n",
    "with open('logistic_regression.pkl', 'wb') as file:\n",
    "    pickle.dump(logistic_model, file)\n",
    "\n",
    "# Export the decision tree model\n",
    "with open('decision_tree.pkl', 'wb') as file:\n",
    "    pickle.dump(tree_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Step 2: Create a Flask App\n",
    "Next, we need to create a Flask application that will serve the machine learning model. In this example, we will create a simple web form that takes input from the user and uses the trained model to make predictions.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from flask import Flask, render_template, request\n",
    "import pickle\n",
    "\n",
    "# Load the logistic regression model\n",
    "with open('logistic_regression.pkl', 'rb') as file:\n",
    "    logistic_model = pickle.load(file)\n",
    "\n",
    "# Load the decision tree model\n",
    "with open('decision_tree.pkl', 'rb') as file:\n",
    "    tree_model = pickle.load(file)\n",
    "\n",
    "# Create the Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define the home page\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('home.html')\n",
    "\n",
    "# Define the prediction page\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get the input values from the form\n",
    "    input_values = [float(x) for x in request.form.values()]\n",
    "\n",
    "    # Make a prediction using the logistic regression model\n",
    "    log_reg_prediction = logistic_model.predict([input_values])[0]\n",
    "\n",
    "    # Make a prediction using the decision tree model\n",
    "    tree_prediction = tree_model.predict([input_values])[0]\n",
    "\n",
    "    # Return the predictions to the user\n",
    "    return render_template('predict.html', log_reg_prediction=log_reg_prediction, tree_prediction=tree_prediction)\n",
    "\n",
    "# Start the Flask application\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Step 3: Create HTML Templates\n",
    "Finally, we need to create HTML templates for the home page and the prediction page. The home page will display a form for the user to enter input values, and the prediction page will display the predictions from the trained models.\n",
    "\n",
    "Here is an example of the home page template (home.html):\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Machine Learning App</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Enter Input Values:</h1>\n",
    "    <form action=\"{{ url_for('predict') }}\" method=\"post\">\n",
    "      <label for=\"value1\">Value 1:</label>\n",
    "      <input type=\"text\" id=\"value1\" name=\"value1\"><br>\n",
    "\n",
    "      <label for=\"value2\">Value 2:</label>\n",
    "      <input type=\"text\" id=\"value2\" name=\"value2\"><br>\n",
    "\n",
    "      <label for=\"value3\">Value 3:</label>\n",
    "      <input type=\"text\" id=\"value3\" name=\"value3\"><br>\n",
    "\n",
    "      <input type=\"submit\" value=\"Submit\">\n",
    "    </form>\n",
    "  </body>\n",
    "</html>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
